{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca4b5a2-7dd9-4fb3-b23f-a426effba890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting robustness\n",
      "  Downloading robustness-1.2.1.post2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tqdm (from robustness)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio (from robustness)\n",
      "  Downloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from robustness) (5.9.6)\n",
      "Collecting gitpython (from robustness)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting py3nvml (from robustness)\n",
      "  Downloading py3nvml-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cox (from robustness)\n",
      "  Downloading cox-0.1.post3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting scikit-learn (from robustness)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting seaborn (from robustness)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from robustness) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from robustness) (0.16.0+cu118)\n",
      "Collecting pandas (from robustness)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from robustness) (1.24.1)\n",
      "Collecting scipy (from robustness)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting GPUtil (from robustness)\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dill (from robustness)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tensorboardX (from robustness)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tables (from robustness)\n",
      "  Downloading tables-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting matplotlib (from robustness)\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->robustness)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->robustness)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->robustness)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->robustness)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->robustness)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->robustness) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->robustness) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->robustness) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->robustness) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->robustness)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->robustness)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting xmltodict (from py3nvml->robustness)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->robustness)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->robustness)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numexpr>=2.6.2 (from tables->robustness)\n",
      "  Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting py-cpuinfo (from tables->robustness)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting blosc2>=2.3.0 (from tables->robustness)\n",
      "  Downloading blosc2-2.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from tables->robustness) (4.4.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->robustness)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->robustness) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->robustness) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->robustness) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->robustness) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->robustness) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->robustness) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->robustness) (2.31.0)\n",
      "Collecting ndindex>=1.4 (from blosc2>=2.3.0->tables->robustness)\n",
      "  Downloading ndindex-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting msgpack (from blosc2>=2.3.0->tables->robustness)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->robustness)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->robustness) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->robustness) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->robustness) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->robustness) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->robustness) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->robustness) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->robustness) (1.3.0)\n",
      "Downloading robustness-1.2.1.post2-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cox-0.1.post3-py3-none-any.whl (18 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m147.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m174.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m181.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tables-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m181.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blosc2-2.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (397 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.3/397.3 kB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading ndindex-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.5/475.5 kB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7395 sha256=a3c242bd1945a9d6e4b8bb7b3cadb5541eecb0b37cc4cc23bcf3fd942dae5e5e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: pytz, py-cpuinfo, GPUtil, xmltodict, tzdata, tqdm, threadpoolctl, smmap, scipy, protobuf, numexpr, ndindex, msgpack, kiwisolver, joblib, grpcio, fonttools, dill, cycler, contourpy, tensorboardX, scikit-learn, py3nvml, pandas, matplotlib, gitdb, blosc2, tables, seaborn, gitpython, cox, robustness\n",
      "Successfully installed GPUtil-1.4.0 blosc2-2.7.1 contourpy-1.3.1 cox-0.1.post3 cycler-0.12.1 dill-0.3.9 fonttools-4.56.0 gitdb-4.0.12 gitpython-3.1.44 grpcio-1.70.0 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 msgpack-1.1.0 ndindex-1.9.2 numexpr-2.10.2 pandas-2.2.3 protobuf-5.29.3 py-cpuinfo-9.0.0 py3nvml-0.2.7 pytz-2025.1 robustness-1.2.1.post2 scikit-learn-1.6.1 scipy-1.15.2 seaborn-0.13.2 smmap-5.0.2 tables-3.10.1 tensorboardX-2.6.2.2 threadpoolctl-3.5.0 tqdm-4.67.1 tzdata-2025.1 xmltodict-0.14.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b613a1b4-e64f-4cac-be18-0c46a0c322c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/usr/local/lib/python3.10/dist-packages/robustness/imagenet_models/__init__.py\") as rf:\n",
    "    contents = rf.read()\n",
    "    contents = contents.replace(\"from .vgg import *\", \"#\")\n",
    "    contents = contents.replace(\"from .alexnet import *\", \"#\")\n",
    "    contents = contents.replace(\"from .squeezenet import *\", \"#\")\n",
    "    with open(\"/usr/local/lib/python3.10/dist-packages/robustness/imagenet_models/__init__.py\", \"w\") as wf:\n",
    "        wf.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd53642-5028-40c4-930f-3d3790850534",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/usr/local/lib/python3.10/dist-packages/robustness/train.py\") as rf:\n",
    "    contents = rf.read()\n",
    "    target_string = '''    for i, (inp, target) in iterator:\n",
    "       # measure data loading time\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        output, final_inp = model(inp, target=target, make_adv=adv,\n",
    "                                  **attack_kwargs)\n",
    "        loss = train_criterion(output, target)'''\n",
    "    replace_string = '''    import random\n",
    "    import copy\n",
    "    for i, (inp, target) in iterator:\n",
    "       # measure data loading time\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        atk_kwargs = {}\n",
    "        atk = random.randint(1, 3)\n",
    "        if atk == 1:\n",
    "            atk_kwargs = copy.deepcopy(attack_kwargs)\n",
    "        output, final_inp = model(inp, target=target, make_adv=adv,\n",
    "                                  **attack_kwargs)\n",
    "        loss = train_criterion(output, target)'''\n",
    "    contents = contents.replace(target_string, replace_string)\n",
    "    with open(\"/usr/local/lib/python3.10/dist-packages/robustness/train.py\", \"w\") as wf:\n",
    "        wf.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48efc870-87cd-4a63-8ffc-1889eaa92dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/workspace/cifar_linf_8.pt'\n",
      "=> loaded checkpoint '/workspace/cifar_linf_8.pt' (epoch 153)\n",
      "==> Preparing dataset cifar..\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /path/to/cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 73643759.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /path/to/cifar/cifar-10-python.tar.gz to /path/to/cifar\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CIFAR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from r50cosine import R50Cosine\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "set_seeds(0)\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n",
    "\n",
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CIFAR('/path/to/cifar')\n",
    "#ds = ImageNet('/path/to/imagenet')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/cifar_linf_8.pt\")\n",
    "#m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"workspace/resnet50_linf_eps8.0.ckpt\")\n",
    "initials = torch.randn([2048, 10])\n",
    "class CosineLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(data=initials.to(device), requires_grad=True)\n",
    "        self.bias = nn.Parameter(data=torch.randn(self.out_features))\n",
    "    def forward(self, x):\n",
    "        #print(torch.norm(x, dim=1))\n",
    "        x = x / torch.norm(x, dim=1, keepdim=True)\n",
    "        params = self.weight / torch.norm(self.params, dim=0)\n",
    "        #concatenated = torch.stack([self.similarity(torch.stack([x[n] for m in range(self.out_features)], dim=1), self.params) for n in torch.arange(x.shape[0])], dim=0)\n",
    "        #return concatenated\n",
    "        return torch.matmul(x, params)\n",
    "#print(m)\n",
    "train_loader, val_loader = ds.make_loaders(batch_size=128, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50151022-4d4c-4d00-98b3-2058c6b147c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'custom_class',\n",
       " 'custom_class_args',\n",
       " 'data_path',\n",
       " 'ds_name',\n",
       " 'get_model',\n",
       " 'label_mapping',\n",
       " 'make_loaders',\n",
       " 'mean',\n",
       " 'num_classes',\n",
       " 'override_args',\n",
       " 'std',\n",
       " 'transform_test',\n",
       " 'transform_train']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30dee81f-0086-4381-8fbb-3707fea958f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader_prep, val_loader_prep = ds.make_loaders(batch_size=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87305a6d-11e9-45a3-aaaa-daac7e806fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0388,  0.0435,  0.0112, -0.0292,  0.0072, -0.0178, -0.0195, -0.0265,\n",
       "          0.0240,  0.0147]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "test_tensor = torch.randn([1, 3, 32, 32]).to(device)\n",
    "m = m.to(device)\n",
    "m(test_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bd1061df-4bff-4850-8c83-562d537b4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(data=initials.to(device), requires_grad=True)\n",
    "        self.bias = nn.Parameter(data=torch.randn(self.out_features), requires_grad=False)\n",
    "    def forward(self, x):\n",
    "        #print(torch.norm(x, dim=1))\n",
    "        weight = torch.permute(self.weight, (1, 0))\n",
    "        x = x / torch.norm(x, dim=1, keepdim=True)\n",
    "        weight = weight / torch.norm(weight, dim=0)\n",
    "        #concatenated = torch.stack([self.similarity(torch.stack([x[n] for m in range(self.out_features)], dim=1), self.params) for n in torch.arange(x.shape[0])], dim=0)\n",
    "        #return concatenated\n",
    "        return torch.matmul(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ef4dcf-bd5e-4d75-808f-60f4ecc1053e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m m\u001b[38;5;241m.\u001b[39mattacker\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m linearlayer\n\u001b[1;32m      9\u001b[0m m \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m m(\u001b[43mtest_tensor\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda')\n",
    "#test_tensor = torch.randn([1, 3, 32, 32]).to(device)\n",
    "initials = torch.randn([10, 2048])\n",
    "linearlayer = nn.Linear(2048, 10)\n",
    "m.model.linear = linearlayer\n",
    "m.attacker.model.linear = linearlayer\n",
    "m = m.to(device)\n",
    "m(test_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b312d573-f867-4706-b45d-f797cbe458b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing label 0\n",
      "Doing label 2\n",
      "Doing label 8\n",
      "Doing label 9\n",
      "Doing label 3\n",
      "Doing label 6\n",
      "Doing label 4\n",
      "Doing label 5\n",
      "Doing label 1\n",
      "Doing label 7\n"
     ]
    }
   ],
   "source": [
    "initials = torch.randn([2048, 10])\n",
    "m.model.linear = nn.Identity()\n",
    "lists = [[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1] for x in range(1)]\n",
    "for (i, data) in enumerate(train_loader):\n",
    "    #print(data[0].shape, data[1])\n",
    "    j = 0\n",
    "    while sum([1 if x == -1 else 0 for x in lists[j]]) != 0:\n",
    "        for k in range(128):\n",
    "            i = 0\n",
    "            #print(data[1])\n",
    "            #index = random.randint(0, 49999)\n",
    "            label = data[1][k].item()\n",
    "            #print(k)\n",
    "            if lists[i][label] == -1:\n",
    "                print(f\"Doing label {label}\")\n",
    "                lists[i][label] = 1\n",
    "                outputs = m(data[0][k].reshape([1, 3, 32, 32]).to(device))[0].detach()\n",
    "                for item in range(2048):\n",
    "                    initials[item][label] = outputs[0][item].detach()\n",
    "initials = torch.permute(initials, (1, 0))\n",
    "m.model.linear = CosineLayer(2048, 10)\n",
    "m.attacker.model.linear = CosineLayer(2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26355a26-0033-4701-a602-5c3811e39c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9720, 0.9371, 0.9895, 0.9827, 0.9788, 0.9855, 0.9666, 0.9807, 0.9792,\n",
      "         0.9540]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for (i, data) in enumerate(train_loader): # not well separated at all, the 10 classes seem bunched together...\n",
    "    test_tensor = data[0][67].unsqueeze(0).to(device)\n",
    "    label = data[1][67]\n",
    "    print(m(test_tensor)[0])\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862fabc6-12be-4661-aa9e-21c52d9c2d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in /usr/local/lib/python3.10/dist-packages (3.10.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from tables) (1.24.1)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables) (2.10.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tables) (23.2)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables) (9.0.0)\n",
      "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from tables) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from tables) (4.4.0)\n",
      "Requirement already satisfied: ndindex>=1.4 in /usr/local/lib/python3.10/dist-packages (from blosc2>=2.3.0->tables) (1.9.2)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2>=2.3.0->tables) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting typing_extensions==4.7.1\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed typing_extensions-4.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tables\n",
    "!pip3 install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd915866-1d0f-4677-9d99-e3ff2d42487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/workspace/cifar_linf_8.pt'\n",
      "=> loaded checkpoint '/workspace/cifar_linf_8.pt' (epoch 153)\n"
     ]
    }
   ],
   "source": [
    "m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/cifar_linf_8.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d1b7ed9-2b83-4489-8147-e631889a7d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/workspace/cifar_linf_8.pt'\n",
      "=> loaded checkpoint '/workspace/cifar_linf_8.pt' (epoch 153)\n"
     ]
    }
   ],
   "source": [
    "baseline_model, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/cifar_linf_8.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d87e6903-61ef-4f03-9c57-029de279abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/workspace/cifar_linf_8.pt'\n",
      "=> loaded checkpoint '/workspace/cifar_linf_8.pt' (epoch 153)\n",
      "mkdir: cannot create directory ‘train_out’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 0.8774 | AdvPrec1 65.712 | AdvPrec5 97.766 | Reg term: 0.0 ||: 100%|██████████| 391/391 [02:41<00:00,  2.42it/s]\n",
      "Val Epoch:0 | Loss 0.4324 | NatPrec1 87.020 | NatPrec5 99.480 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.11it/s]\n",
      "Val Epoch:0 | Loss 1.2626 | AdvPrec1 54.460 | AdvPrec5 95.300 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:29<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Hard-coded base parameters\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/cifar_linf_8.pt\")\n",
    "#m.model.linear = nn.Linear(2048, 10)\n",
    "#m.attacker.model.linear = nn.Linear(2048, 10)\n",
    "%mkdir train_out\n",
    "clean_train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'epochs': 4,\n",
    "    'lr': 0.002, # at this checkpoint don't use an excessively high learning rate (0.001 is usually enough), or else it overfits to that threat model\n",
    "    'adv_train': 0,\n",
    "    'adv_eval': 1,\n",
    "    'constraint': 'inf',\n",
    "    'eps': 8/255,\n",
    "    'attack_lr': 1.5,\n",
    "    'attack_steps': 20\n",
    "}\n",
    "clean_train_args = Parameters(clean_train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "clean_train_args = defaults.check_and_fill_args(clean_train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "clean_train_args = defaults.check_and_fill_args(clean_train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)\n",
    "adv_train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'epochs': 1,\n",
    "    'lr': 0.002, # at this checkpoint don't use an excessively high learning rate (0.001 is usually enough), or else it overfits to that threat model\n",
    "    'adv_train': 1,\n",
    "    'adv_eval': 1,\n",
    "    'constraint': 'inf',\n",
    "    'eps': 8/255,\n",
    "    'attack_lr': 2/255,\n",
    "    'attack_steps': 10\n",
    "}\n",
    "adv_train_args = Parameters(adv_train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)\n",
    "\n",
    "# Train a model\n",
    "try:\n",
    "    m = train.train_model(adv_train_args, m, (train_loader, val_loader))\n",
    "except AssertionError:\n",
    "    m = train.train_model(adv_train_args, m.module, (train_loader, val_loader))\n",
    "#m = train.train_model(adv_train_args, m.module, (train_loader, val_loader))\n",
    "#train.train_model(adv_train_args, m, (train_loader, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40c8c878-0e7b-44dc-8365-5d1834092223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch:0 | Loss 0.4324 | NatPrec1 87.020 | NatPrec5 99.480 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 47.67it/s]\n",
      "Val Epoch:0 | Loss 1.2875 | AdvPrec1 53.700 | AdvPrec5 95.380 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:56<00:00,  1.41it/s]\n",
      "Val Epoch:0 | Loss 0.4324 | NatPrec1 87.020 | NatPrec5 99.480 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.29it/s]\n",
      "Val Epoch:0 | Loss 1.2978 | AdvPrec1 53.310 | AdvPrec5 95.420 | Reg term: 0.0 ||: 100%|██████████| 79/79 [04:27<00:00,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from robustness.train import eval_model\n",
    "#m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/cifar_linf_8.pt\")\n",
    "adv_train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'epochs': 1,\n",
    "    'lr': 0.002, # at this checkpoint don't use an excessively high learning rate (0.001 is usually enough), or else it overfits to that threat model\n",
    "    'adv_train': 1,\n",
    "    'adv_eval': 1,\n",
    "    'constraint': 'inf',\n",
    "    'eps': 8/255,\n",
    "    'attack_lr': 2/255,\n",
    "    'attack_steps': 20\n",
    "}\n",
    "adv_train_args = Parameters(adv_train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)\n",
    "try:\n",
    "    eval_model(adv_train_args, m.module, val_loader, None)\n",
    "except AttributeError:\n",
    "    eval_model(adv_train_args, m, val_loader, None)\n",
    "adv_train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'epochs': 1,\n",
    "    'lr': 0.002, # at this checkpoint don't use an excessively high learning rate (0.001 is usually enough), or else it overfits to that threat model\n",
    "    'adv_train': 1,\n",
    "    'adv_eval': 1,\n",
    "    'constraint': 'inf',\n",
    "    'eps': 8/255,\n",
    "    'attack_lr': 2/255,\n",
    "    'attack_steps': 100\n",
    "}\n",
    "adv_train_args = Parameters(adv_train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)\n",
    "try:\n",
    "    eval_model(adv_train_args, m.module, val_loader, None)\n",
    "except AttributeError:\n",
    "    eval_model(adv_train_args, m, val_loader, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb709f85-345e-4642-89ef-6c4dd5c23188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchattacks\n",
      "  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\n",
      "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.16.0+cu118)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.67.1)\n",
      "Collecting requests~=2.25.1 (from torchattacks)\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.24.1)\n",
      "Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->torchattacks) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\n",
      "Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: idna, chardet, requests, torchattacks\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.25.0 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a82a9-b50b-4121-b5b7-b749af1f36d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a44ad1-a3cf-4f26-a897-2c46340c8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = None\n",
    "def evaluate_attacked_model(loader, phase, atk, limit=5000, model=net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    counter = 0\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        #print(labels)\n",
    "        net.train()\n",
    "        images = atk(images, labels)\n",
    "        net.eval()\n",
    "        outputs = model(images.to(device))\n",
    "        #print(outputs.shape)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        #print(outputs.data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total >= limit:\n",
    "            break\n",
    "    print(f'{phase} accuracy: {100 * correct / total} ({correct} / {total}) %')\n",
    "    return correct, total_loss / math.ceil(limit / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88c620e3-7986-4fb8-aa0b-9e75e83977d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 86.96 (8696 / 10000) %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8696, 0.17230087623000145)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchattacks\n",
    "set_seeds(0)\n",
    "device = torch.device('cuda')\n",
    "model = nn.Sequential(baseline_model.normalizer, baseline_model.model).to(device)\n",
    "net = nn.Sequential(m.module.normalizer, m.module.model).to(device)\n",
    "pgd10 = torchattacks.PGD(net, steps=20)\n",
    "pgd10 = torchattacks.VANILA(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "import math\n",
    "evaluate_attacked_model(val_loader, \"test\", pgd10, limit=10000, model=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c3ca7-886c-4f16-8134-525093729120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchattacks baseline: 54.27% PGD-10, 53.01% PGD-20\n",
    "# torchattacks baseline + 1-epoch PGD5: 53.04% PGD-20\n",
    "# torchattacks net on baseline: 54.70% PGD-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c405541d-5067-4ad1-b06e-ed1dbf1cf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), \"53.570_pgd20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d5764d5-6c76-485d-ae9e-c2ea2661e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict_backup = m.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc7c64d-fa4b-470a-bdaa-7e024082ad2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.load_state_dict(model_state_dict_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d88eae-9ac7-426d-88e8-c54749245671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"/workspace/cifar_linf_8.pt\")[\"state_dict\"]\n",
    "model_state_dict = m.state_dict()\n",
    "import copy\n",
    "new_state_dict = copy.deepcopy(state_dict)\n",
    "for item in state_dict.keys():\n",
    "    if \"attacker\" in item:\n",
    "        model_state_dict[item] = state_dict[item]\n",
    "m.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e36d6bb3-47dc-4fc6-81da-69b9831b64ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/workspace/cifar_linf_8.pt'\n",
      "=> loaded checkpoint '/workspace/cifar_linf_8.pt' (epoch 153)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 0.9503 | AdvPrec1 61.016 | AdvPrec5 97.578 | Reg term: 0.0 ||:   3%|▎         | 10/391 [00:07<04:59,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m adv_train_args \u001b[38;5;241m=\u001b[39m defaults\u001b[38;5;241m.\u001b[39mcheck_and_fill_args(adv_train_args,\n\u001b[1;32m     17\u001b[0m                         defaults\u001b[38;5;241m.\u001b[39mTRAINING_ARGS, CIFAR)\n\u001b[1;32m     18\u001b[0m adv_train_args \u001b[38;5;241m=\u001b[39m defaults\u001b[38;5;241m.\u001b[39mcheck_and_fill_args(adv_train_args,\n\u001b[1;32m     19\u001b[0m                         defaults\u001b[38;5;241m.\u001b[39mPGD_ARGS, CIFAR)\n\u001b[0;32m---> 20\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43madv_train_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/robustness/train.py:311\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(args, model, loaders, checkpoint, dp_device_ids, store, update_params, disable_no_grad)\u001b[0m\n\u001b[1;32m    307\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     train_prec1, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43m_model_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     last_epoch \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m==\u001b[39m (args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/robustness/train.py:453\u001b[0m, in \u001b[0;36m_model_loop\u001b[0;34m(args, loop_type, loader, model, opt, epoch, adv, writer)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m atk \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    452\u001b[0m     atk_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(attack_kwargs)\n\u001b[0;32m--> 453\u001b[0m output, final_inp \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_adv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattack_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_criterion(output, target)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loss\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/robustness/attacker.py:313\u001b[0m, in \u001b[0;36mAttackerModel.forward\u001b[0;34m(self, inp, target, make_adv, with_latent, fake_relu, no_relu, with_image, **attacker_kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m prev_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 313\u001b[0m adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacker\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattacker_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_training:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/robustness/attacker.py:249\u001b[0m, in \u001b[0;36mAttacker.forward\u001b[0;34m(self, x, target, constraint, eps, step_size, iterations, random_start, random_restarts, do_tqdm, targeted, custom_loss, should_normalize, orig_input, use_best, return_image, est_grad, mixed_precision, *_)\u001b[0m\n\u001b[1;32m    247\u001b[0m     adv_ret \u001b[38;5;241m=\u001b[39m to_ret\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 249\u001b[0m     adv_ret \u001b[38;5;241m=\u001b[39m \u001b[43mget_adv_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/robustness/attacker.py:213\u001b[0m, in \u001b[0;36mAttacker.forward.<locals>.get_adv_examples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    212\u001b[0m     args \u001b[38;5;241m=\u001b[39m [losses, best_loss, x, best_x]\n\u001b[0;32m--> 213\u001b[0m     best_loss, best_x \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_best\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m use_best \u001b[38;5;28;01melse\u001b[39;00m (losses, x)\n\u001b[1;32m    215\u001b[0m     x \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mstep(x, grad)\n\u001b[1;32m    216\u001b[0m     x \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mproject(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/robustness/attacker.py:183\u001b[0m, in \u001b[0;36mAttacker.forward.<locals>.get_adv_examples.<locals>.replace_best\u001b[0;34m(loss, bloss, x, bx)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     replace \u001b[38;5;241m=\u001b[39m m \u001b[38;5;241m*\u001b[39m bloss \u001b[38;5;241m<\u001b[39m m \u001b[38;5;241m*\u001b[39m loss\n\u001b[0;32m--> 183\u001b[0m     bx[replace] \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    184\u001b[0m     bloss[replace] \u001b[38;5;241m=\u001b[39m loss[replace]\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bloss, bx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'epochs': 1,\n",
    "    'lr': 0, # at this checkpoint don't use an excessively high learning rate (0.001 is usually enough), or else it overfits to that threat model\n",
    "    'adv_train': 0,\n",
    "    'adv_eval': 1,\n",
    "    'constraint': 'inf',\n",
    "    'eps': 8/255,\n",
    "    'attack_lr': 2/255,\n",
    "    'attack_steps': 20\n",
    "}\n",
    "adv_train_args = Parameters(adv_train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "adv_train_args = defaults.check_and_fill_args(adv_train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)\n",
    "m = train.train_model(adv_train_args, m, (train_loader, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1816d714-9c3c-426b-b7da-64adb7c50819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchattacks\n",
      "  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\n",
      "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.16.0+cu118)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.67.1)\n",
      "Collecting requests~=2.25.1 (from torchattacks)\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.24.1)\n",
      "Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->torchattacks) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\n",
      "Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: idna, chardet, requests, torchattacks\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.25.0 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "36e8b5d3-51a5-48b3-8a5c-6b6b6c73b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 2.2048 | NatPrec1 81.294 | NatPrec5 99.656 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 17.97it/s]\n",
      "Val Epoch:0 | Loss 2.1968 | NatPrec1 78.040 | NatPrec5 99.250 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 47.87it/s]\n",
      "Val Epoch:0 | Loss 2.2261 | AdvPrec1 51.480 | AdvPrec5 91.830 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.45it/s]\n",
      "Train Epoch:0 | Loss 2.2108 | AdvPrec1 59.958 | AdvPrec5 95.432 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 2.1793 | NatPrec1 78.940 | NatPrec5 99.230 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.58it/s]\n",
      "Val Epoch:0 | Loss 2.2114 | AdvPrec1 52.820 | AdvPrec5 92.430 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.45it/s]\n",
      "Train Epoch:0 | Loss 2.1649 | NatPrec1 85.046 | NatPrec5 99.668 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 17.95it/s]\n",
      "Val Epoch:0 | Loss 2.1570 | NatPrec1 81.080 | NatPrec5 99.270 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.34it/s]\n",
      "Val Epoch:0 | Loss 2.1966 | AdvPrec1 52.910 | AdvPrec5 92.050 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.45it/s]\n",
      "Train Epoch:0 | Loss 2.1778 | AdvPrec1 61.888 | AdvPrec5 95.442 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.36it/s]\n",
      "Val Epoch:0 | Loss 2.1391 | NatPrec1 81.120 | NatPrec5 99.160 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.62it/s]\n",
      "Val Epoch:0 | Loss 2.1814 | AdvPrec1 53.770 | AdvPrec5 92.410 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.1225 | NatPrec1 86.718 | NatPrec5 99.682 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.01it/s]\n",
      "Val Epoch:0 | Loss 2.1150 | NatPrec1 82.910 | NatPrec5 99.300 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.08it/s]\n",
      "Val Epoch:0 | Loss 2.1664 | AdvPrec1 52.900 | AdvPrec5 92.040 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.1435 | AdvPrec1 62.208 | AdvPrec5 95.352 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 2.0973 | NatPrec1 82.510 | NatPrec5 99.130 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.98it/s]\n",
      "Val Epoch:0 | Loss 2.1505 | AdvPrec1 54.260 | AdvPrec5 92.160 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.45it/s]\n",
      "Train Epoch:0 | Loss 2.0782 | NatPrec1 87.678 | NatPrec5 99.674 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.25it/s]\n",
      "Val Epoch:0 | Loss 2.0718 | NatPrec1 84.220 | NatPrec5 99.270 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.25it/s]\n",
      "Val Epoch:0 | Loss 2.1361 | AdvPrec1 53.800 | AdvPrec5 92.090 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.1086 | AdvPrec1 62.504 | AdvPrec5 95.208 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.36it/s]\n",
      "Val Epoch:0 | Loss 2.0542 | NatPrec1 83.870 | NatPrec5 99.140 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 50.43it/s]\n",
      "Val Epoch:0 | Loss 2.1201 | AdvPrec1 54.210 | AdvPrec5 92.170 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.0331 | NatPrec1 88.702 | NatPrec5 99.630 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.30it/s]\n",
      "Val Epoch:0 | Loss 2.0278 | NatPrec1 85.370 | NatPrec5 99.250 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.62it/s]\n",
      "Val Epoch:0 | Loss 2.1064 | AdvPrec1 53.580 | AdvPrec5 91.840 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.0733 | AdvPrec1 62.892 | AdvPrec5 95.190 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 2.0114 | NatPrec1 84.330 | NatPrec5 99.150 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.16it/s]\n",
      "Val Epoch:0 | Loss 2.0898 | AdvPrec1 54.210 | AdvPrec5 92.150 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.9882 | NatPrec1 89.362 | NatPrec5 99.662 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.18it/s]\n",
      "Val Epoch:0 | Loss 1.9843 | NatPrec1 86.130 | NatPrec5 99.330 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.73it/s]\n",
      "Val Epoch:0 | Loss 2.0786 | AdvPrec1 53.370 | AdvPrec5 91.680 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.0387 | AdvPrec1 63.250 | AdvPrec5 95.364 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.36it/s]\n",
      "Val Epoch:0 | Loss 1.9696 | NatPrec1 85.150 | NatPrec5 99.140 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.47it/s]\n",
      "Val Epoch:0 | Loss 2.0609 | AdvPrec1 54.570 | AdvPrec5 91.970 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.9444 | NatPrec1 90.096 | NatPrec5 99.628 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.40it/s]\n",
      "Val Epoch:0 | Loss 1.9425 | NatPrec1 86.710 | NatPrec5 99.260 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.55it/s]\n",
      "Val Epoch:0 | Loss 2.0513 | AdvPrec1 52.930 | AdvPrec5 91.490 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 2.0046 | AdvPrec1 63.870 | AdvPrec5 95.442 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:45<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 1.9292 | NatPrec1 85.610 | NatPrec5 99.090 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 51.91it/s]\n",
      "Val Epoch:0 | Loss 2.0318 | AdvPrec1 54.990 | AdvPrec5 92.040 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.9017 | NatPrec1 90.606 | NatPrec5 99.690 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.12it/s]\n",
      "Val Epoch:0 | Loss 1.9018 | NatPrec1 87.060 | NatPrec5 99.390 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.06it/s]\n",
      "Val Epoch:0 | Loss 2.0242 | AdvPrec1 53.740 | AdvPrec5 91.710 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.9708 | AdvPrec1 64.490 | AdvPrec5 95.678 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 1.8909 | NatPrec1 85.950 | NatPrec5 99.140 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.48it/s]\n",
      "Val Epoch:0 | Loss 2.0038 | AdvPrec1 55.420 | AdvPrec5 92.330 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.8614 | NatPrec1 91.270 | NatPrec5 99.714 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.10it/s]\n",
      "Val Epoch:0 | Loss 1.8630 | NatPrec1 87.680 | NatPrec5 99.450 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.97it/s]\n",
      "Val Epoch:0 | Loss 1.9978 | AdvPrec1 54.100 | AdvPrec5 92.080 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.9354 | AdvPrec1 66.042 | AdvPrec5 96.064 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:46<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 1.8537 | NatPrec1 86.570 | NatPrec5 99.150 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 48.92it/s]\n",
      "Val Epoch:0 | Loss 1.9716 | AdvPrec1 57.040 | AdvPrec5 92.970 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.8228 | NatPrec1 91.856 | NatPrec5 99.718 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:21<00:00, 18.25it/s]\n",
      "Val Epoch:0 | Loss 1.8262 | NatPrec1 88.470 | NatPrec5 99.430 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 49.26it/s]\n",
      "Val Epoch:0 | Loss 1.9640 | AdvPrec1 56.450 | AdvPrec5 93.030 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n",
      "Train Epoch:0 | Loss 1.8943 | AdvPrec1 69.038 | AdvPrec5 96.862 | Reg term: 0.0 ||: 100%|██████████| 391/391 [04:45<00:00,  1.37it/s]\n",
      "Val Epoch:0 | Loss 1.8180 | NatPrec1 87.350 | NatPrec5 99.160 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:01<00:00, 51.29it/s]\n",
      "Val Epoch:0 | Loss 1.9339 | AdvPrec1 60.260 | AdvPrec5 94.090 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:54<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in range(10):\n",
    "    m = train.train_model(clean_train_args, m.module, (train_loader, val_loader))\n",
    "    # TODO: do a torchattacks PGD evaluation here\n",
    "    m = train.train_model(adv_train_args, m.module, (train_loader, val_loader))\n",
    "    # TODO: do a torchattacks PGD evaluation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199b5a0f-84ab-4792-a817-d0e416ee6823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/workspace/cifar_linf_8.pt'\n",
      "=> loaded checkpoint '/workspace/cifar_linf_8.pt' (epoch 153)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Wrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "    def forward(self, x):\n",
    "        output = self.base_model(x)\n",
    "        #print(type(output[0]))\n",
    "        return output[0]\n",
    "net = Wrapper(m)\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n",
    "\n",
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CIFAR('/path/to/cifar')\n",
    "#m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/train_out/checkpoint.pt.latest\")\n",
    "#m.model.linear = CosineLayer(2048, 10)\n",
    "#m.attacker.model.linear = CosineLayer(2048, 10)\n",
    "m2, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds, resume_path=\"/workspace/cifar_linf_8.pt\")\n",
    "net = Wrapper(m)\n",
    "net2 = Wrapper(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "563b2fdb-53a1-4cdd-991b-3f9678ffe062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attacked_model(loader, phase, atk, limit=5000, model=net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    counter = 0\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        #print(labels)\n",
    "        net.train()\n",
    "        images = atk(images, labels)\n",
    "        net.eval()\n",
    "        outputs = model(images)\n",
    "        #print(outputs.shape)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        #print(outputs.data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total >= limit:\n",
    "            break\n",
    "    print(f'{phase} accuracy: {100 * correct / total} ({correct} / {total}) %')\n",
    "    return correct, total_loss / math.ceil(limit / 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98aa0667-e4b1-4daf-a570-7959f1ff1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_atk accuracy: 54.53 (5453 / 10000) %\n",
      "test_atk accuracy: 49.88 (4988 / 10000) %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4988, 0.17665645103454589)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchattacks\n",
    "atk = torchattacks.PGD(net)\n",
    "#atk = torchattacks.AutoAttack(net)\n",
    "#atk.set_normalization_used((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "atk2 = torchattacks.PGD(net2)\n",
    "#atk2 = torchattacks.AutoAttack(net2)\n",
    "import torch\n",
    "import math\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda')\n",
    "evaluate_attacked_model(val_loader, \"test_atk\", atk, limit=10000, model=net)\n",
    "evaluate_attacked_model(val_loader, \"test_atk\", atk2, limit=10000, model=net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7538668-d68f-4648-aa5e-3afc03e59410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple\n",
    "from typing_extensions import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2c42add-ec9c-4510-8182-791a6ed7a4cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Callable' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtables\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tables/__init__.py:51\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlosc2 library not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI looked for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(blosc2_search_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Necessary imports to get versions stored on the cython extension\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilsextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_hdf5_version \u001b[38;5;28;01mas\u001b[39;00m _get_hdf5_version\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     55\u001b[0m hdf5_version \u001b[38;5;241m=\u001b[39m _get_hdf5_version()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tables/utilsextension.pyx:25\u001b[0m, in \u001b[0;36minit tables.utilsextension\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tables/description.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Generator, Literal, Optional, Sequence, Type, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m atom\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_name_validity\n\u001b[1;32m     13\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreStructuredText\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tables/atom.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, dataclass_transform, Dict, NoReturn, Union\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Callable' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17064536-060b-409c-aebe-d792a248f5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
